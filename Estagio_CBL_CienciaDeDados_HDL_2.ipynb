{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBL8c26NRdyNZwTwz7Rpm/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jana-nf/Estagio_CBL_CienciaDeDados_HDL/blob/main/Estagio_CBL_CienciaDeDados_HDL_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjust file paths\n",
        "Subtask:\n",
        "Create the necessary directory, move the customer CSV file into it, and then execute the CSV to JSON conversion function.\n",
        "\n",
        "Reasoning: The previous attempt to convert CSV to JSON failed because the input file was not found in the RAW_DATA_PATH. This step will create the RAW_DATA_PATH directory if it doesn't exist, move the olist_customers_dataset.csv file into it, and then call the convert_csv_to_json() function to ensure the conversion happens with the correct file path."
      ],
      "metadata": {
        "id": "fHLKwVuRsMaY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "rq1etL5vsGNA",
        "outputId": "a5cda5dd-0711-4c49-9da8-9f6982a3b096"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1317823210.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Create the raw data directory if it doesn't exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRAW_DATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Ensured directory exists: {RAW_DATA_PATH}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "# Create the raw data directory if it doesn't exist\n",
        "os.makedirs(RAW_DATA_PATH, exist_ok=True)\n",
        "print(f\"Ensured directory exists: {RAW_DATA_PATH}\")\n",
        "\n",
        "# Define source and destination paths for the customer CSV file\n",
        "source_path = os.path.join(BASE_DIR, INPUT_CSV)\n",
        "destination_path = os.path.join(RAW_DATA_PATH, INPUT_CSV)\n",
        "\n",
        "# Check if the file needs to be moved\n",
        "if os.path.exists(source_path) and not os.path.exists(destination_path):\n",
        "    shutil.move(source_path, destination_path)\n",
        "    print(f\"Moved '{INPUT_CSV}' from '{BASE_DIR}' to '{RAW_DATA_PATH}'.\")\n",
        "elif os.path.exists(destination_path):\n",
        "    print(f\"'{INPUT_CSV}' already exists in '{RAW_DATA_PATH}'. Skipping move.\")\n",
        "elif not os.path.exists(source_path):\n",
        "    print(f\"Error: '{INPUT_CSV}' not found at '{BASE_DIR}'. Cannot move file.\")\n",
        "\n",
        "# Execute the CSV to JSON conversion function\n",
        "convert_csv_to_json()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjust file paths and create SQLite DB for order items\n",
        "Subtask:\n",
        "Create the necessary directory, move the order items CSV file into it, and then execute the SQLite database creation function.\n",
        "\n",
        "Reasoning: To resolve the FileNotFoundError for 'olist_order_items_dataset.csv', I will ensure the target directory exists, move the CSV file into it if it's not already there, and then call the create_sqlite_db() function to process the file and create the SQLite database."
      ],
      "metadata": {
        "id": "58VHXoIcsmYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Create the raw data directory if it doesn't exist\n",
        "os.makedirs(RAW_DATA_PATH, exist_ok=True)\n",
        "print(f\"Ensured directory exists: {RAW_DATA_PATH}\")\n",
        "\n",
        "# Define source and destination paths for the order items CSV file\n",
        "source_path_items = os.path.join(BASE_DIR, INPUT_CSV_ITEMS)\n",
        "destination_path_items = os.path.join(RAW_DATA_PATH, INPUT_CSV_ITEMS)\n",
        "\n",
        "# Check if the file needs to be moved\n",
        "if os.path.exists(source_path_items) and not os.path.exists(destination_path_items):\n",
        "    shutil.move(source_path_items, destination_path_items)\n",
        "    print(f\"Moved '{INPUT_CSV_ITEMS}' from '{BASE_DIR}' to '{RAW_DATA_PATH}'.\")\n",
        "elif os.path.exists(destination_path_items):\n",
        "    print(f\"'{INPUT_CSV_ITEMS}' already exists in '{RAW_DATA_PATH}'. Skipping move.\")\n",
        "elif not os.path.exists(source_path_items):\n",
        "    print(f\"Error: '{INPUT_CSV_ITEMS}' not found at '{BASE_DIR}'. Please ensure the file is present before attempting to move it.\")\n",
        "\n",
        "# Execute the SQLite database creation function\n",
        "create_sqlite_db()"
      ],
      "metadata": {
        "id": "DKyvU32YsbBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integrate core data sources\n",
        "Subtask:\n",
        "Load customer data from JSON, order items from SQLite, and begin merging with the orders DataFrame.\n",
        "\n",
        "Reasoning: The subtask requires loading customer data from a JSON file, order items from an SQLite database, and then merging these with the existing orders DataFrame. This code block will perform all these data loading and merging operations sequentially, finally displaying the head of the integrated DataFrame."
      ],
      "metadata": {
        "id": "KHtpy-ZUsx5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import sqlite3\n",
        "import os\n",
        "\n",
        "# 1. Load customer data from JSON\n",
        "path_customer_json = os.path.join(RAW_DATA_PATH, OUTPUT_JSON)\n",
        "print(f\"Loading customer data from: {path_customer_json}\")\n",
        "with open(path_customer_json, 'r') as f:\n",
        "    customers_data = json.load(f)\n",
        "df_customers_json = pd.DataFrame(customers_data)\n",
        "\n",
        "# 2. Connect to the SQLite database and 3. Read the order_items table\n",
        "path_sqlite_db = os.path.join(RAW_DATA_PATH, OUTPUT_DB_NAME)\n",
        "print(f\"Connecting to SQLite database: {path_sqlite_db}\")\n",
        "conn = sqlite3.connect(path_sqlite_db)\n",
        "df_order_items_sql = pd.read_sql_query(f\"SELECT * FROM {SQL_TABLE_NAME}\", conn)\n",
        "\n",
        "# 4. Close the SQLite database connection\n",
        "conn.close()\n",
        "print(\"SQLite connection closed.\")\n",
        "\n",
        "# Ensure all DataFrames are loaded\n",
        "print(f\"df_orders shape: {df_orders.shape}\")\n",
        "print(f\"df_customers_json shape: {df_customers_json.shape}\")\n",
        "print(f\"df_order_items_sql shape: {df_order_items_sql.shape}\")\n",
        "\n",
        "# 5. Merge df_orders with df_customers_json on 'customer_id'\n",
        "df_merged_data = pd.merge(df_orders, df_customers_json, on='customer_id', how='left')\n",
        "print(f\"Shape after merging df_orders and df_customers_json: {df_merged_data.shape}\")\n",
        "\n",
        "# 6. Merge df_merged_data with df_order_items_sql on 'order_id'\n",
        "df_merged_data = pd.merge(df_merged_data, df_order_items_sql, on='order_id', how='left')\n",
        "print(f\"Shape after merging with df_order_items_sql: {df_merged_data.shape}\")\n",
        "\n",
        "# 7. Display the first few rows of df_merged_data\n",
        "print(\"\\nFirst few rows of the integrated DataFrame (df_merged_data):\")\n",
        "df_merged_data.head()"
      ],
      "metadata": {
        "id": "qfjtgcThs12G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integrate remaining order-related data\n",
        "Subtask:\n",
        "Merge df_order_payments and df_order_reviews into the df_merged_data DataFrame.\n",
        "\n",
        "Reasoning: To integrate the remaining order-related data, I will merge df_order_payments and df_order_reviews into the df_merged_data DataFrame sequentially, and then display its shape and head."
      ],
      "metadata": {
        "id": "R5aRhyxVs8Ri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shape before merging df_order_payments: {df_merged_data.shape}\")\n",
        "df_merged_data = pd.merge(df_merged_data, df_order_payments, on='order_id', how='left')\n",
        "print(f\"Shape after merging df_order_payments: {df_merged_data.shape}\")\n",
        "\n",
        "print(f\"Shape before merging df_order_reviews: {df_merged_data.shape}\")\n",
        "df_merged_data = pd.merge(df_merged_data, df_order_reviews, on='order_id', how='left')\n",
        "print(f\"Shape after merging df_order_reviews: {df_merged_data.shape}\")\n",
        "\n",
        "print(\"\\nFirst few rows of the integrated DataFrame (df_merged_data) after all merges:\")\n",
        "df_merged_data.head()"
      ],
      "metadata": {
        "id": "em_3dPNus_9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integrate product and seller data\n",
        "Subtask:\n",
        "Merge df_products, df_product_category_name_translation, and df_sellers into the df_merged_data DataFrame.\n",
        "\n",
        "Reasoning: To integrate the remaining product and seller data, I will merge df_products, df_product_category_name_translation, and df_sellers into the df_merged_data DataFrame sequentially, and then display its shape and head."
      ],
      "metadata": {
        "id": "EzxPw-bstKWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shape before merging df_products: {df_merged_data.shape}\")\n",
        "df_merged_data = pd.merge(df_merged_data, df_products, on='product_id', how='left')\n",
        "print(f\"Shape after merging df_products: {df_merged_data.shape}\")\n",
        "\n",
        "print(f\"Shape before merging df_product_category_name_translation: {df_merged_data.shape}\")\n",
        "df_merged_data = pd.merge(df_merged_data, df_product_category_name_translation, on='product_category_name', how='left')\n",
        "print(f\"Shape after merging df_product_category_name_translation: {df_merged_data.shape}\")\n",
        "\n",
        "print(f\"Shape before merging df_sellers: {df_merged_data.shape}\")\n",
        "df_merged_data = pd.merge(df_merged_data, df_sellers, on='seller_id', how='left')\n",
        "print(f\"Shape after merging df_sellers: {df_merged_data.shape}\")\n",
        "\n",
        "print(\"\\nFirst few rows of the integrated DataFrame (df_merged_data) after all merges:\")\n",
        "df_merged_data.head()"
      ],
      "metadata": {
        "id": "PPmEa7bctK-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean and preprocess merged data\n",
        "Subtask:\n",
        "Handle missing values and convert data types in the df_merged_data DataFrame.\n",
        "\n",
        "Reasoning: The first step is to inspect the df_merged_data DataFrame for missing values and current data types using .info() and .isnull().sum() to understand the extent of missingness and guide subsequent cleaning steps, as per the instructions."
      ],
      "metadata": {
        "id": "WwuGg__JtPHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"DataFrame Info:\")\n",
        "df_merged_data.info()\n",
        "\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df_merged_data.isnull().sum())"
      ],
      "metadata": {
        "id": "REJFPhm-tUOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datetime_cols = [\n",
        "    'order_purchase_timestamp',\n",
        "    'order_approved_at',\n",
        "    'order_delivered_carrier_date',\n",
        "    'order_delivered_customer_date',\n",
        "    'order_estimated_delivery_date',\n",
        "    'shipping_limit_date',\n",
        "    'review_creation_date',\n",
        "    'review_answer_timestamp'\n",
        "]\n",
        "\n",
        "for col in datetime_cols:\n",
        "    # Using errors='coerce' will turn unparseable dates into NaT (Not a Time)\n",
        "    df_merged_data[col] = pd.to_datetime(df_merged_data[col], errors='coerce')\n",
        "\n",
        "print(\"Date and time columns converted to datetime objects.\")\n",
        "print(\"Updated DataFrame Info:\")\n",
        "df_merged_data.info()"
      ],
      "metadata": {
        "id": "VBl47OHeteOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reasoning: Now that datetime columns are handled, I will proceed to address missing values and convert data types for numerical columns, specifically focusing on product dimensions, payment details, and review scores. This involves identifying the columns, analyzing their current data types, and applying a suitable imputation strategy as outlined in the instructions."
      ],
      "metadata": {
        "id": "GfnJ1doetlvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_cols_to_clean = [\n",
        "    'product_name_lenght',\n",
        "    'product_description_lenght',\n",
        "    'product_photos_qty',\n",
        "    'product_weight_g',\n",
        "    'product_length_cm',\n",
        "    'product_height_cm',\n",
        "    'product_width_cm',\n",
        "    'payment_sequential',\n",
        "    'payment_installments',\n",
        "    'payment_value',\n",
        "    'review_score',\n",
        "    'order_item_id',\n",
        "    'price',\n",
        "    'freight_value',\n",
        "    'seller_zip_code_prefix' # Assuming this is treated numerically for simplicity or conversion later\n",
        "]\n",
        "\n",
        "for col in numerical_cols_to_clean:\n",
        "    if df_merged_data[col].isnull().any():\n",
        "        # For counts/lengths, median might be more robust to outliers than mean\n",
        "        # For IDs or sequential numbers, fill with 0 or a placeholder if appropriate,\n",
        "        # but for these, median/mean is often fine if they represent a quantity.\n",
        "        # Let's use median for most, and 0 for payment_sequential if a missing value implies no payment info.\n",
        "        if col == 'payment_sequential':\n",
        "            df_merged_data[col] = df_merged_data[col].fillna(0).astype(int) # 0 for missing payment seq\n",
        "        elif col == 'order_item_id':\n",
        "            df_merged_data[col] = df_merged_data[col].fillna(1).astype(int) # Assuming 1 item if missing\n",
        "        elif col in ['product_name_lenght', 'product_description_lenght', 'product_photos_qty', 'product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm']:\n",
        "            df_merged_data[col] = df_merged_data[col].fillna(df_merged_data[col].median()).astype(int)\n",
        "        elif col == 'review_score':\n",
        "            df_merged_data[col] = df_merged_data[col].fillna(df_merged_data[col].median()).astype(int) # Fill with median score\n",
        "        elif col in ['price', 'freight_value']:\n",
        "            df_merged_data[col] = df_merged_data[col].fillna(df_merged_data[col].mean()).astype(float) # Mean for monetary values\n",
        "        elif col == 'seller_zip_code_prefix':\n",
        "            df_merged_data[col] = df_merged_data[col].fillna(df_merged_data[col].mode()[0]).astype(int) # Mode for zip code\n",
        "    else:\n",
        "        # Convert to appropriate type if no NaNs, or if NaNs were already handled by previous steps\n",
        "        if col in ['product_name_lenght', 'product_description_lenght', 'product_photos_qty', 'product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm', 'payment_sequential', 'payment_installments', 'review_score', 'order_item_id', 'seller_zip_code_prefix']:\n",
        "            # Ensure they are integer type after handling potential NaNs\n",
        "            df_merged_data[col] = df_merged_data[col].astype(int)\n",
        "        elif col in ['price', 'freight_value', 'payment_value']:\n",
        "            df_merged_data[col] = df_merged_data[col].astype(float)\n",
        "\n",
        "\n",
        "print(\"Numerical columns processed (NaNs imputed and types converted).\")\n",
        "print(\"Updated DataFrame Info:\")\n",
        "df_merged_data.info()"
      ],
      "metadata": {
        "id": "h77y51NmtoJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reasoning: Following the instructions, I will now address the missing values in categorical columns. For product_category_name, product_category_name_english, review_comment_title, and review_comment_message, I will fill NaN values with the placeholder 'unknown' to retain information and prevent data loss, which is a common practice for categorical features"
      ],
      "metadata": {
        "id": "gX28G_tdtxnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_cols_to_clean = [\n",
        "    'product_category_name',\n",
        "    'product_category_name_english',\n",
        "    'review_comment_title',\n",
        "    'review_comment_message',\n",
        "    'payment_type',\n",
        "    'product_id',\n",
        "    'seller_id',\n",
        "    'seller_city',\n",
        "    'seller_state',\n",
        "    'review_id'\n",
        "]\n",
        "\n",
        "for col in categorical_cols_to_clean:\n",
        "    if df_merged_data[col].isnull().any():\n",
        "        if col == 'product_id':\n",
        "            df_merged_data[col] = df_merged_data[col].fillna('unknown_product')\n",
        "        elif col == 'seller_id':\n",
        "            df_merged_data[col] = df_merged_data[col].fillna('unknown_seller')\n",
        "        elif col == 'payment_type':\n",
        "            df_merged_data[col] = df_merged_data[col].fillna(df_merged_data[col].mode()[0])\n",
        "        elif col == 'review_id':\n",
        "            df_merged_data[col] = df_merged_data[col].fillna('unknown_review') # Placeholder for review_id\n",
        "        else:\n",
        "            df_merged_data[col] = df_merged_data[col].fillna('unknown')\n",
        "\n",
        "print(\"Categorical columns processed (NaNs imputed).\")\n",
        "print(\"Updated DataFrame Info:\")\n",
        "df_merged_data.info()\n",
        "\n",
        "print(\"\\nMissing values per column after categorical processing:\")\n",
        "print(df_merged_data[categorical_cols_to_clean].isnull().sum())\n"
      ],
      "metadata": {
        "id": "P1DYHQXUtyPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reasoning: The previous step handled categorical column NaNs. Now, I will address the remaining missing values in numerical columns (payment_installments, payment_value) by filling them with their respective medians and converting their data types. I will also fill the missing values in datetime columns using the mode, as these represent specific event times that would be best approximated by the most frequent occurrence when missing, ensuring all critical columns are clean."
      ],
      "metadata": {
        "id": "4QUnul3nt59G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Handle remaining numerical NaNs\n",
        "# payment_installments: fill with median and convert to int\n",
        "if df_merged_data['payment_installments'].isnull().any():\n",
        "    df_merged_data['payment_installments'] = df_merged_data['payment_installments'].fillna(df_merged_data['payment_installments'].median()).astype(int)\n",
        "\n",
        "# payment_value: fill with mean and convert to float\n",
        "if df_merged_data['payment_value'].isnull().any():\n",
        "    df_merged_data['payment_value'] = df_merged_data['payment_value'].fillna(df_merged_data['payment_value'].mean()).astype(float)\n",
        "\n",
        "# Handle remaining datetime NaNs\n",
        "datetime_cols_with_nans = [\n",
        "    'order_approved_at',\n",
        "    'order_delivered_carrier_date',\n",
        "    'order_delivered_customer_date',\n",
        "    'shipping_limit_date',\n",
        "    'review_creation_date',\n",
        "    'review_answer_timestamp'\n",
        "]\n",
        "\n",
        "for col in datetime_cols_with_nans:\n",
        "    if df_merged_data[col].isnull().any():\n",
        "        # Fill with the mode (most frequent datetime) for each column\n",
        "        # mode() returns a Series, take the first element if there are multiple modes\n",
        "        mode_value = df_merged_data[col].mode()[0]\n",
        "        df_merged_data[col] = df_merged_data[col].fillna(mode_value)\n",
        "\n",
        "\n",
        "print(\"Remaining NaNs in numerical and datetime columns handled.\")\n",
        "print(\"Updated DataFrame Info:\")\n",
        "df_merged_data.info()\n",
        "\n",
        "print(\"\\nMissing values per column after all cleaning steps:\")\n",
        "print(df_merged_data.isnull().sum()[df_merged_data.isnull().sum() > 0])"
      ],
      "metadata": {
        "id": "jPe4w4mKt6Za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carregar o dataset limpo em um arquivo final (ex: clean_data.csv)\n",
        "Subtask:\n",
        "Save the cleaned and merged DataFrame (df_merged_data) to a CSV file.\n",
        "\n",
        "Reasoning: To complete the subtask, I will define the output path and filename for the cleaned data, then save the df_merged_data DataFrame to a CSV file at that location, and finally print a confirmation message."
      ],
      "metadata": {
        "id": "Hrc6WPcBt-pZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_CLEAN_CSV = 'clean_data.csv'\n",
        "PATH_CLEAN_CSV = os.path.join(RAW_DATA_PATH, OUTPUT_CLEAN_CSV)\n",
        "\n",
        "# Save the cleaned and merged DataFrame to a CSV file\n",
        "df_merged_data.to_csv(PATH_CLEAN_CSV, index=False)\n",
        "\n",
        "print(f\"Successfully saved cleaned data to: {PATH_CLEAN_CSV}\")"
      ],
      "metadata": {
        "id": "PTF4LGCWuCl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Automação: Garantir que este processo seja executado por um único script modular.\n",
        "Subtask:\n",
        "Consolidate all ETL steps (extraction, transformation, loading) into a single, modular Python script.\n",
        "\n",
        "Reasoning: To consolidate all ETL steps into a single, modular Python script as requested, I will provide the complete Python code for etl_pipeline.py. This script will include all necessary imports, global variables, data loading from various sources (CSV, JSON, SQLite), data merging, cleaning, feature engineering, and finally, saving the processed data to a new CSV file. This fulfills the subtask's requirement to create a unified and executable ETL pipeline."
      ],
      "metadata": {
        "id": "Qjl6jNEouI50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import sqlite3\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "# --- Global Configurations ---\n",
        "BASE_DIR = os.getcwd()\n",
        "RAW_DATA_PATH = os.path.join(BASE_DIR, 'data', '01_raw')\n",
        "\n",
        "# Input/Output file names and table names\n",
        "INPUT_CUSTOMERS_CSV = 'olist_customers_dataset.csv'\n",
        "INPUT_ORDER_ITEMS_CSV = 'olist_order_items_dataset.csv'\n",
        "INPUT_ORDERS_CSV = 'olist_orders_dataset.csv'\n",
        "INPUT_ORDER_PAYMENTS_CSV = 'olist_order_payments_dataset.csv'\n",
        "INPUT_ORDER_REVIEWS_CSV = 'olist_order_reviews_dataset.csv'\n",
        "INPUT_PRODUCTS_CSV = 'olist_products_dataset.csv'\n",
        "INPUT_SELLERS_CSV = 'olist_sellers_dataset.csv'\n",
        "INPUT_PRODUCT_CATEGORY_TRANSLATION_CSV = 'product_category_name_translation.csv'\n",
        "\n",
        "OUTPUT_CUSTOMERS_JSON = 'clientes_api.json'\n",
        "OUTPUT_ORDER_ITEMS_DB = 'estoque.db'\n",
        "SQL_TABLE_NAME = 'order_items'\n",
        "OUTPUT_CLEAN_CSV = 'clean_data.csv'\n",
        "\n",
        "def ensure_raw_data_path_and_move_files():\n",
        "    \"\"\"Ensures the RAW_DATA_PATH exists and moves specific CSVs into it.\"\"\"\n",
        "    os.makedirs(RAW_DATA_PATH, exist_ok=True)\n",
        "    print(f\"Ensured directory exists: {RAW_DATA_PATH}\")\n",
        "\n",
        "    files_to_move = [\n",
        "        INPUT_CUSTOMERS_CSV,\n",
        "        INPUT_ORDER_ITEMS_CSV,\n",
        "        INPUT_ORDERS_CSV,\n",
        "        INPUT_ORDER_PAYMENTS_CSV,\n",
        "        INPUT_ORDER_REVIEWS_CSV,\n",
        "        INPUT_PRODUCTS_CSV,\n",
        "        INPUT_SELLERS_CSV,\n",
        "        INPUT_PRODUCT_CATEGORY_TRANSLATION_CSV\n",
        "    ]\n",
        "\n",
        "    for filename in files_to_move:\n",
        "        source_path = os.path.join(BASE_DIR, filename)\n",
        "        destination_path = os.path.join(RAW_DATA_PATH, filename)\n",
        "        if os.path.exists(source_path) and not os.path.exists(destination_path):\n",
        "            shutil.move(source_path, destination_path)\n",
        "            print(f\"Moved '{filename}' from '{BASE_DIR}' to '{RAW_DATA_PATH}'.\")\n",
        "        elif os.path.exists(destination_path):\n",
        "            print(f\"'{filename}' already exists in '{RAW_DATA_PATH}'. Skipping move.\")\n",
        "        elif not os.path.exists(source_path):\n",
        "            print(f\"Warning: '{filename}' not found at '{BASE_DIR}'. Cannot move file.\")\n",
        "\n",
        "def convert_csv_to_json():\n",
        "    \"\"\"Lê o CSV de clientes e salva como JSON para simular uma fonte de API.\"\"\"\n",
        "\n",
        "    path_input_csv = os.path.join(RAW_DATA_PATH, INPUT_CUSTOMERS_CSV)\n",
        "    path_output_json = os.path.join(RAW_DATA_PATH, OUTPUT_CUSTOMERS_JSON)\n",
        "\n",
        "    print(f\"Lendo CSV de clientes em: {path_input_csv}\")\n",
        "\n",
        "    try:\n",
        "        df_customers = pd.read_csv(path_input_csv)\n",
        "        df_customers.to_json(\n",
        "            path_output_json,\n",
        "            orient='records',\n",
        "            date_format='iso'\n",
        "        )\n",
        "        print(f\"Sucesso! {len(df_customers)} registros convertidos para JSON.\")\n",
        "        print(f\"Arquivo JSON salvo em: {path_output_json}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Ocorreu um erro durante a conversão CSV para JSON: {e}\")\n",
        "\n",
        "def create_sqlite_db():\n",
        "    \"\"\"Lê o CSV de itens e carrega os dados em uma tabela SQLite local.\"\"\"\n",
        "\n",
        "    path_input_csv = os.path.join(RAW_DATA_PATH, INPUT_ORDER_ITEMS_CSV)\n",
        "    path_output_db = os.path.join(RAW_DATA_PATH, OUTPUT_ORDER_ITEMS_DB)\n",
        "\n",
        "    print(f\"Lendo CSV de itens em: {path_input_csv}\")\n",
        "\n",
        "    try:\n",
        "        df_items = pd.read_csv(path_input_csv)\n",
        "        conn = sqlite3.connect(path_output_db)\n",
        "\n",
        "        print(f\"Carregando {len(df_items)} registros na tabela '{SQL_TABLE_NAME}'...\")\n",
        "\n",
        "        df_items.to_sql(\n",
        "            name=SQL_TABLE_NAME,\n",
        "            con=conn,\n",
        "            if_exists='replace',\n",
        "            index=False\n",
        "        )\n",
        "\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(f\"SELECT COUNT(*) FROM {SQL_TABLE_NAME}\")\n",
        "        count = cursor.fetchone()[0]\n",
        "        conn.close()\n",
        "\n",
        "        print(f\"Sucesso! Banco de dados '{OUTPUT_ORDER_ITEMS_DB}' criado com {count} linhas na tabela.\")\n",
        "        print(f\"Arquivo DB salvo em: {path_output_db}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Ocorreu um erro ao criar o banco de dados SQLite: {e}\")\n",
        "\n",
        "def run_etl_pipeline():\n",
        "    \"\"\"Executes the complete ETL pipeline.\"\"\"\n",
        "    print(\"\\n--- Starting ETL Pipeline ---\")\n",
        "\n",
        "    # Step 1: Ensure directory and move raw files\n",
        "    ensure_raw_data_path_and_move_files()\n",
        "\n",
        "    # Step 2: Extraction - Create JSON and SQLite from raw CSVs\n",
        "    convert_csv_to_json()\n",
        "    create_sqlite_db()\n",
        "\n",
        "    # Step 3: Load all data sources into DataFrames\n",
        "    print(\"\\nLoading dataframes...\")\n",
        "    try:\n",
        "        df_orders = pd.read_csv(os.path.join(RAW_DATA_PATH, INPUT_ORDERS_CSV))\n",
        "        df_order_payments = pd.read_csv(os.path.join(RAW_DATA_PATH, INPUT_ORDER_PAYMENTS_CSV))\n",
        "        df_order_reviews = pd.read_csv(os.path.join(RAW_DATA_PATH, INPUT_ORDER_REVIEWS_CSV))\n",
        "        df_products = pd.read_csv(os.path.join(RAW_DATA_PATH, INPUT_PRODUCTS_CSV))\n",
        "        df_sellers = pd.read_csv(os.path.join(RAW_DATA_PATH, INPUT_SELLERS_CSV))\n",
        "        df_product_category_name_translation = pd.read_csv(os.path.join(RAW_DATA_PATH, INPUT_PRODUCT_CATEGORY_TRANSLATION_CSV))\n",
        "\n",
        "        path_customer_json = os.path.join(RAW_DATA_PATH, OUTPUT_CUSTOMERS_JSON)\n",
        "        with open(path_customer_json, 'r') as f:\n",
        "            customers_data = json.load(f)\n",
        "        df_customers_json = pd.DataFrame(customers_data)\n",
        "\n",
        "        path_sqlite_db = os.path.join(RAW_DATA_PATH, OUTPUT_ORDER_ITEMS_DB)\n",
        "        conn = sqlite3.connect(path_sqlite_db)\n",
        "        df_order_items_sql = pd.read_sql_query(f\"SELECT * FROM {SQL_TABLE_NAME}\", conn)\n",
        "        conn.close()\n",
        "        print(\"All dataframes loaded successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading initial dataframes: {e}\")\n",
        "        return\n",
        "\n",
        "    # Step 4: Merging all DataFrames\n",
        "    print(\"\\nMerging dataframes...\")\n",
        "    df_merged_data = pd.merge(df_orders, df_customers_json, on='customer_id', how='left')\n",
        "    df_merged_data = pd.merge(df_merged_data, df_order_items_sql, on='order_id', how='left')\n",
        "    df_merged_data = pd.merge(df_merged_data, df_order_payments, on='order_id', how='left')\n",
        "    df_merged_data = pd.merge(df_merged_data, df_order_reviews, on='order_id', how='left')\n",
        "    df_merged_data = pd.merge(df_merged_data, df_products, on='product_id', how='left')\n",
        "    df_merged_data = pd.merge(df_merged_data, df_product_category_name_translation, on='product_category_name', how='left')\n",
        "    df_merged_data = pd.merge(df_merged_data, df_sellers, on='seller_id', how='left')\n",
        "    print(f\"Dataframes merged. Final shape: {df_merged_data.shape}\")\n",
        "\n",
        "    # Step 5: Data Cleaning and Preprocessing\n",
        "    print(\"\\nCleaning and preprocessing data...\")\n",
        "    # Convert datetime columns\n",
        "    datetime_cols = [\n",
        "        'order_purchase_timestamp',\n",
        "        'order_approved_at',\n",
        "        'order_delivered_carrier_date',\n",
        "        'order_delivered_customer_date',\n",
        "        'order_estimated_delivery_date',\n",
        "        'shipping_limit_date',\n",
        "        'review_creation_date',\n",
        "        'review_answer_timestamp'\n",
        "    ]\n",
        "    for col in datetime_cols:\n",
        "        df_merged_data[col] = pd.to_datetime(df_merged_data[col], errors='coerce')\n",
        "\n",
        "    # Handle numerical NaNs\n",
        "    numerical_cols_to_clean = [\n",
        "        'product_name_lenght',\n",
        "        'product_description_lenght',\n",
        "        'product_photos_qty',\n",
        "        'product_weight_g',\n",
        "        'product_length_cm',\n",
        "        'product_height_cm',\n",
        "        'product_width_cm',\n",
        "        'payment_sequential',\n",
        "        'payment_installments',\n",
        "        'payment_value',\n",
        "        'review_score',\n",
        "        'order_item_id',\n",
        "        'price',\n",
        "        'freight_value',\n",
        "        'seller_zip_code_prefix'\n",
        "    ]\n",
        "\n",
        "    for col in numerical_cols_to_clean:\n",
        "        if df_merged_data[col].isnull().any():\n",
        "            if col == 'payment_sequential':\n",
        "                df_merged_data[col] = df_merged_data[col].fillna(0).astype(int)\n",
        "            elif col == 'order_item_id':\n",
        "                df_merged_data[col] = df_merged_data[col].fillna(1).astype(int)\n",
        "            elif col in ['product_name_lenght', 'product_description_lenght', 'product_photos_qty', 'product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm']:\n",
        "                df_merged_data[col] = df_merged_data[col].fillna(df_merged_data[col].median()).astype(int)\n",
        "            elif col == 'review_score':\n",
        "                df_merged_data[col] = df_merged_data[col].fillna(df_merged_data[col].median()).astype(int)\n",
        "            elif col in ['price', 'freight_value']:\n",
        "                df_merged_data[col] = df_merged_data[col].fillna(df_merged_data[col].mean()).astype(float)\n",
        "            elif col == 'seller_zip_code_prefix':\n",
        "                df_merged_data[col] = df_merged_data[col].fillna(df_merged_data[col].mode()[0]).astype(int)\n",
        "        else:\n",
        "            if col in ['product_name_lenght', 'product_description_lenght', 'product_photos_qty', 'product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm', 'payment_sequential', 'payment_installments', 'review_score', 'order_item_id', 'seller_zip_code_prefix']:\n",
        "                df_merged_data[col] = df_merged_data[col].astype(int)\n",
        "            elif col in ['price', 'freight_value', 'payment_value']:\n",
        "                df_merged_data[col] = df_merged_data[col].astype(float)\n",
        "\n",
        "    # Handle categorical NaNs\n",
        "    categorical_cols_to_clean = [\n",
        "        'product_category_name',\n",
        "        'product_category_name_english',\n",
        "        'review_comment_title',\n",
        "        'review_comment_message',\n",
        "        'payment_type',\n",
        "        'product_id',\n",
        "        'seller_id',\n",
        "        'seller_city',\n",
        "        'seller_state',\n",
        "        'review_id'\n",
        "    ]\n",
        "    for col in categorical_cols_to_clean:\n",
        "        if df_merged_data[col].isnull().any():\n",
        "            if col == 'product_id':\n",
        "                df_merged_data[col] = df_merged_data[col].fillna('unknown_product')\n",
        "            elif col == 'seller_id':\n",
        "                df_merged_data[col] = df_merged_data[col].fillna('unknown_seller')\n",
        "            elif col == 'payment_type':\n",
        "                df_merged_data[col] = df_merged_data[col].fillna(df_merged_data[col].mode()[0])\n",
        "            elif col == 'review_id':\n",
        "                df_merged_data[col] = df_merged_data[col].fillna('unknown_review')\n",
        "            else:\n",
        "                df_merged_data[col] = df_merged_data[col].fillna('unknown')\n",
        "\n",
        "    # Handle remaining numerical and datetime NaNs\n",
        "    if df_merged_data['payment_installments'].isnull().any():\n",
        "        df_merged_data['payment_installments'] = df_merged_data['payment_installments'].fillna(df_merged_data['payment_installments'].median()).astype(int)\n",
        "    if df_merged_data['payment_value'].isnull().any():\n",
        "        df_merged_data['payment_value'] = df_merged_data['payment_value'].fillna(df_merged_data['payment_value'].mean()).astype(float)\n",
        "\n",
        "    datetime_cols_with_nans = [\n",
        "        'order_approved_at',\n",
        "        'order_delivered_carrier_date',\n",
        "        'order_delivered_customer_date',\n",
        "        'shipping_limit_date',\n",
        "        'review_creation_date',\n",
        "        'review_answer_timestamp'\n",
        "    ]\n",
        "    for col in datetime_cols_with_nans:\n",
        "        if df_merged_data[col].isnull().any():\n",
        "            mode_value = df_merged_data[col].mode()[0]\n",
        "            df_merged_data[col] = df_merged_data[col].fillna(mode_value)\n",
        "\n",
        "    print(\"Data cleaning and preprocessing complete.\")\n",
        "\n",
        "    # Step 6: Calculate delivery time and product value metrics\n",
        "    print(\"\\nCalculating new features...\")\n",
        "    df_merged_data['delivery_time_days'] = (df_merged_data['order_delivered_customer_date'] - df_merged_data['order_purchase_timestamp']).dt.days\n",
        "    df_merged_data['product_total_value'] = df_merged_data['price'] * df_merged_data['order_item_id']\n",
        "    print(\"New features calculated.\")\n",
        "\n",
        "    # Step 7: Save the final cleaned and transformed DataFrame\n",
        "    path_clean_csv = os.path.join(RAW_DATA_PATH, OUTPUT_CLEAN_CSV)\n",
        "    df_merged_data.to_csv(path_clean_csv, index=False)\n",
        "    print(f\"\\nSuccessfully saved cleaned data to: {path_clean_csv}\")\n",
        "    print(\"--- ETL Pipeline Finished ---\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_etl_pipeline()\n"
      ],
      "metadata": {
        "id": "vKTG2XLIuMaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reasoning: Now that datetime columns are handled, I will proceed to address missing values and convert data types for numerical columns, specifically focusing on product dimensions, payment details, and review scores. This involves identifying the columns, analyzing their current data types, and applying a suitable imputation strategy as outlined in the instructions."
      ],
      "metadata": {
        "id": "cmp3lcmsuYO3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8SR8eWmruZAu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}